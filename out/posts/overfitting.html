<!DOCTYPE html><html lang="en"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png"/><link rel="manifest" href="/favicon/site.webmanifest"/><link rel="mask-icon" href="/favicon/safari-pinned-tab.svg" color="#000000"/><link rel="shortcut icon" href="/favicon/favicon.ico"/><meta name="msapplication-TileColor" content="#000000"/><meta name="msapplication-config" content="/favicon/browserconfig.xml"/><meta name="theme-color" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><meta property="og:image" content="https://og-image.now.sh/Next.js%20Blog%20Starter%20Example.png?theme=light&amp;md=1&amp;fontSize=100px&amp;images=https%3A%2F%2Fassets.vercel.com%2Fimage%2Fupload%2Ffront%2Fassets%2Fdesign%2Fnextjs-black-logo.svg"/><title>Thoughts on the Human Aspect of Overfitting | Next.js Blog Example with Markdown</title><meta property="og:image" content="/assets/blog/overfitting/cover.jpg"/><meta name="next-head-count" content="15"/><link rel="preload" href="/_next/static/css/3a6b687d1a5d5f581a9f.css" as="style"/><link rel="stylesheet" href="/_next/static/css/3a6b687d1a5d5f581a9f.css"/><link rel="preload" href="/_next/static/css/12e6371168c7cf0f8455.css" as="style"/><link rel="stylesheet" href="/_next/static/css/12e6371168c7cf0f8455.css"/><link rel="preload" href="/_next/static/chunks/main-f01cf1dcca173d7f43b1.js" as="script"/><link rel="preload" href="/_next/static/chunks/webpack-ccf5ab034a524403276a.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.ea5d6f7a7099b14097ba.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.136473f176143783e714.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-f346660bb478e2d855ff.js" as="script"/><link rel="preload" href="/_next/static/chunks/2c3b9bffc7150b37a7b3810dfcf1c0b0fac2bd18.a154912b5d5a068d0e7e.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/posts/%5Bslug%5D-a33c22cf7a1b1bd07131.js" as="script"/></head><body><div id="__next"><div class="min-h-screen"><main><div class="container mx-auto px-5"><h2 class="text-2xl md:text-4xl font-bold tracking-tight md:tracking-tighter leading-tight mb-8 mt-8"><a class="hover:underline" href="/">Thoughts</a><hr class="mt-1"/></h2><article class="mb-32"><h3 class="text-4xl md:text-5xl lg:text-6xl font-bold tracking-tighter leading-tight md:leading-none mb-6 text-center md:text-left">Thoughts on the Human Aspect of Overfitting</h3><div class="hidden md:block md:mb-12"><div class="flex items-center"><img src="/assets/blog/authors/galen.jpg" class="w-12 h-12 rounded-full mr-4" alt="Galen Caldwell"/><div class="text-xl font-bold">Galen Caldwell</div></div></div><div class="mb-8 md:mb-16 mx-0"><div><img src="/assets/blog/overfitting/cover.jpg" alt="Cover Image for Thoughts on the Human Aspect of Overfitting" width="300" class="shadow-small"/></div></div><div class="max-w-2xl mx-auto"><div class="block md:hidden mb-6"><div class="flex items-center"><img src="/assets/blog/authors/galen.jpg" class="w-12 h-12 rounded-full mr-4" alt="Galen Caldwell"/><div class="text-xl font-bold">Galen Caldwell</div></div></div><div class="mb-6 text-lg"><time dateTime="2020-08-26T12:30:00.322Z">August	26, 2020</time></div></div><div class="max-w-2xl mx-auto"><div class="markdown-styles_markdown__1x9gM"><p>While overfitting is classically seen in data science as a technical phenomenon that can be solved through proper technique, i.e. split testing/training data, k-fold cross validation, etc… A recent Twitter exchange between Nate Silver and G. Elliot Morris has me thinking about it differently. Silver is quibbling over people claiming back-testing as a historical record for accuracy, saying: “A model should receive zero credit/blame for predictive accuracy until it makes actual out-of-sample predictions that are disclosed ahead of time.” I didn’t actually find the Twitter thread particularly useful, but it did get me thinking about the human aspect of overfitting. </p>
<p>The tweet the started it all:</p>
<blockquote class="twitter-tweet" data-theme="light"><p lang="en" dir="ltr">It is generally better to think more carefully about how to pragmatically account for real-world uncertainty in a forecast than to not think carefully about it but then engage in a whole lot of existential conversation about it.</p>&mdash; Nate Silver (@NateSilver538) <a href="https://twitter.com/NateSilver538/status/1292254053867622401?ref_src=twsrc%5Etfw">August 9, 2020</a></blockquote> 
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>In school, I learned about overfitting as something computers do when training models. You see, computers are dumb, they do exactly what you tell them to do. So, if you tell them to find a model that exactly fits the data, they may arrive at ridiculous conclusions. Say you try to model the roll of a six-sided die, and you include data about the day of the week the die was rolled. Odds are your data set won’t perfectly match the distribution for a six-sided die. In this hypothetical example, let’s say there were a few more 4’s than expected. Luckily the computer has an explanation at hand! If you take the days we rolled extra fours, and say that fours are slightly more likely to be rolled on this day, you can have a perfect distribution! A person would never do this, so the discussion of overfitting in class was about making the computer “smarter” to prevent it from making such a silly mistake.</p>
<p>The discussion of overfitting gets narrowed in the classroom because the focus of the class is on techniques to make machine learning algorithms more effective. If you take the textbook definition, it is much broader: To quote <em>Artificial Intelligence, A Modern Approach:</em> “Whenever there is a large set of possible hypotheses, one has to be careful not to use the resulting freedom to find meaningless "regularity" in the data.“ There is no mention of a computer here. In the context of the Twitter thread, where they are discussing modeling the presidential election, there is quite a bit of freedom afforded to the people making the model. In complex models, people must make decisions about input to the model, which has drastic effect. Taking efforts to minimize the human assumptions can result in phenomenal models, see the GPT-2 language model for instance, which solves a wide set of natural language problems effectively because it assumes so little about the input. However, these approaches require lots of data, which is not available for a presidential election. We only have good data on the last dozen or so elections.</p>
<p>What does this mean practically? How might you avoid “human overfitting”? In Nate’s case, he says he avoids checking the result until the final debugging stage. He doesn’t want his perception of events to influence the decisions when making a model, and influencing the output based on his priors. This still doesn’t address the issue of overfitting to past events with an existing model. Unfortunately, for a situation like a presidential election, with so little data, this is not a “solved” problem. As far as I can tell, the state of the art is still based on gut feeling and best effort.</p>
<p>Citations:<br>
1. You find Nate Silver, Editor-in-Chief of FiveThirtyEight.com on Twitter <a class="citation-link" href="https://twitter.com/NateSilver538">here</a><br>
2. You find G. Elliot Morris, Data Journalist at The Economist on Twitter <a class="citation-link" href="https://twitter.com/gelliottmorris">here</a><br>
3. Peter Norvig and Stuart J. Russell. <em>Artificial Intelligence, A Modern Approach</em>. Prentice-Hall, 1995.  </p>
</div></div></article></div></main></div><footer class="bg-accent-1 border-t border-accent-2"><div class="container mx-auto px-5"></div></footer></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Thoughts on the Human Aspect of Overfitting","date":"2020-08-26T12:30:00.322Z","slug":"overfitting","author":{"name":"Galen Caldwell","picture":"/assets/blog/authors/galen.jpg"},"content":"\u003cp\u003eWhile overfitting is classically seen in data science as a technical phenomenon that can be solved through proper technique, i.e. split testing/training data, k-fold cross validation, etc… A recent Twitter exchange between Nate Silver and G. Elliot Morris has me thinking about it differently. Silver is quibbling over people claiming back-testing as a historical record for accuracy, saying: “A model should receive zero credit/blame for predictive accuracy until it makes actual out-of-sample predictions that are disclosed ahead of time.” I didn’t actually find the Twitter thread particularly useful, but it did get me thinking about the human aspect of overfitting. \u003c/p\u003e\n\u003cp\u003eThe tweet the started it all:\u003c/p\u003e\n\u003cblockquote class=\"twitter-tweet\" data-theme=\"light\"\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eIt is generally better to think more carefully about how to pragmatically account for real-world uncertainty in a forecast than to not think carefully about it but then engage in a whole lot of existential conversation about it.\u003c/p\u003e\u0026mdash; Nate Silver (@NateSilver538) \u003ca href=\"https://twitter.com/NateSilver538/status/1292254053867622401?ref_src=twsrc%5Etfw\"\u003eAugust 9, 2020\u003c/a\u003e\u003c/blockquote\u003e \n\u003cscript async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"\u003e\u003c/script\u003e\n\u003cp\u003eIn school, I learned about overfitting as something computers do when training models. You see, computers are dumb, they do exactly what you tell them to do. So, if you tell them to find a model that exactly fits the data, they may arrive at ridiculous conclusions. Say you try to model the roll of a six-sided die, and you include data about the day of the week the die was rolled. Odds are your data set won’t perfectly match the distribution for a six-sided die. In this hypothetical example, let’s say there were a few more 4’s than expected. Luckily the computer has an explanation at hand! If you take the days we rolled extra fours, and say that fours are slightly more likely to be rolled on this day, you can have a perfect distribution! A person would never do this, so the discussion of overfitting in class was about making the computer “smarter” to prevent it from making such a silly mistake.\u003c/p\u003e\n\u003cp\u003eThe discussion of overfitting gets narrowed in the classroom because the focus of the class is on techniques to make machine learning algorithms more effective. If you take the textbook definition, it is much broader: To quote \u003cem\u003eArtificial Intelligence, A Modern Approach:\u003c/em\u003e “Whenever there is a large set of possible hypotheses, one has to be careful not to use the resulting freedom to find meaningless \"regularity\" in the data.“ There is no mention of a computer here. In the context of the Twitter thread, where they are discussing modeling the presidential election, there is quite a bit of freedom afforded to the people making the model. In complex models, people must make decisions about input to the model, which has drastic effect. Taking efforts to minimize the human assumptions can result in phenomenal models, see the GPT-2 language model for instance, which solves a wide set of natural language problems effectively because it assumes so little about the input. However, these approaches require lots of data, which is not available for a presidential election. We only have good data on the last dozen or so elections.\u003c/p\u003e\n\u003cp\u003eWhat does this mean practically? How might you avoid “human overfitting”? In Nate’s case, he says he avoids checking the result until the final debugging stage. He doesn’t want his perception of events to influence the decisions when making a model, and influencing the output based on his priors. This still doesn’t address the issue of overfitting to past events with an existing model. Unfortunately, for a situation like a presidential election, with so little data, this is not a “solved” problem. As far as I can tell, the state of the art is still based on gut feeling and best effort.\u003c/p\u003e\n\u003cp\u003eCitations:\u003cbr\u003e\n1. You find Nate Silver, Editor-in-Chief of FiveThirtyEight.com on Twitter \u003ca class=\"citation-link\" href=\"https://twitter.com/NateSilver538\"\u003ehere\u003c/a\u003e\u003cbr\u003e\n2. You find G. Elliot Morris, Data Journalist at The Economist on Twitter \u003ca class=\"citation-link\" href=\"https://twitter.com/gelliottmorris\"\u003ehere\u003c/a\u003e\u003cbr\u003e\n3. Peter Norvig and Stuart J. Russell. \u003cem\u003eArtificial Intelligence, A Modern Approach\u003c/em\u003e. Prentice-Hall, 1995.  \u003c/p\u003e\n","ogImage":{"url":"/assets/blog/overfitting/cover.jpg"},"coverImage":"/assets/blog/overfitting/cover.jpg"}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"overfitting"},"buildId":"9qju-40LRfJPDHXRLDoO9","nextExport":false,"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-9297741ddbbb81bca141.js"></script><script src="/_next/static/chunks/main-f01cf1dcca173d7f43b1.js" async=""></script><script src="/_next/static/chunks/webpack-ccf5ab034a524403276a.js" async=""></script><script src="/_next/static/chunks/framework.ea5d6f7a7099b14097ba.js" async=""></script><script src="/_next/static/chunks/commons.136473f176143783e714.js" async=""></script><script src="/_next/static/chunks/pages/_app-f346660bb478e2d855ff.js" async=""></script><script src="/_next/static/chunks/2c3b9bffc7150b37a7b3810dfcf1c0b0fac2bd18.a154912b5d5a068d0e7e.js" async=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-a33c22cf7a1b1bd07131.js" async=""></script><script src="/_next/static/9qju-40LRfJPDHXRLDoO9/_buildManifest.js" async=""></script><script src="/_next/static/9qju-40LRfJPDHXRLDoO9/_ssgManifest.js" async=""></script></body></html>